{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install datasets evaluate transformers[sentencepiece]\n",
    "# ! pip install accelerate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "# model_checkpoint = \"distilbert-base-uncased\"\n",
    "model_checkpoint = \"google-bert/bert-base-multilingual-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a great [MASK].\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> This is a great success.'\n",
      "'>>> This is a great thing.'\n",
      "'>>> This is a great feat.'\n",
      "'>>> This is a great adventure.'\n",
      "'>>> This is a great deal.'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "token_logits = model(**inputs).logits\n",
    "\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['निदान आप शायद नहीं जानते होंगे कि आपको एट्रियल फ़िब्रिलेशन (एएफआईबी) है', 'किसी अन्य कारण से स्वास्थ्य जांच कराने पर इस स्थिति का पता चल सकता है', 'यह त्वरित और दर्द रहित परीक्षण हृदय की विद्युत गतिविधि को मापता है']\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/100sentences_hin.txt\", 'r', encoding='utf-8') as f:\n",
    "    sent = [line.strip() for line in f]\n",
    "\n",
    "print(type(sent))\n",
    "print(sent[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import Dataset\n",
    "# from datasets import DatasetDict\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train, val, _, _ = train_test_split(sent, sent, train_size=0.8, random_state=1)\n",
    "\n",
    "# train = [{\"text\": sentence} for sentence in train]\n",
    "# train = Dataset.from_list(train)\n",
    "\n",
    "# val = [{\"text\": sentence} for sentence in val]\n",
    "# val = Dataset.from_list(val)\n",
    "\n",
    "# dataset = DatasetDict({\n",
    "#     \"train\": train,\n",
    "#     \"validation\": val\n",
    "# })\n",
    "\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "train = train = [{\"text\": sentence} for sentence in sent]\n",
    "train = Dataset.from_list(train)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train,\n",
    "    # \"validation\": val\n",
    "})\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> क्या आप जो दवा लिख ​​रहे हैं उसका कोई सामान्य विकल्प है?'\n",
      ">>> रक्त को पतला करने वाली दवाओं में वारफारिन जांटोवेन, एपिक्साबैन एलिकिस, डाबीगेट्रान प्रदाक्सा, एडोक्साबैन सावेसा और रिवरोक्साबैन ज़ेरेल्टो शामिल हैं।'\n",
      ">>> आपके साथ जाने वाला कोई व्यक्ति आपको दी गई जानकारी याद रखने में मदद कर सकता है'\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[\"train\"].shuffle(seed=42).select(range(3))\n",
    "\n",
    "for row in sample:\n",
    "    print(f\">>> {row['text']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bece192f15814be3a44ac7ff3cc18a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\"]\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> sentence 0 length: 37'\n",
      "'>>> sentence 1 length: 21'\n",
      "'>>> sentence 2 length: 28'\n"
     ]
    }
   ],
   "source": [
    "tokenized_samples = tokenized_datasets[\"train\"][:3]\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"'>>> sentence {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Concatenated sentences length: 86'\n"
     ]
    }
   ],
   "source": [
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated sentences length: {total_length}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Chunk length: 86'\n"
     ]
    }
   ],
   "source": [
    "chunks = {\n",
    "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729d96aa4a1b47ce963985814a6fd641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 24\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'स जोडत ह, जो परीकषण परिणामो को परिट या परदरशित करता ह [SEP] [CLS] जब आप अपनी नियमित गतिविधिया करत ह तो इस एक या दो दिन क लिए पहना जाता ह [SEP] [CLS] अनियमित हदय ताल का पता चलन पर कछ उपकरण सवचालित रप स रिकॉरड करत ह [SEP] [CLS] यह उपकरण तीन साल तक लगातार दिल की धडकन को रिकॉरड करता ह [SEP] [CLS] डिवाइस दिखाता ह कि जब आप अपनी दनिक गतिवि'"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [CLS] [MASK]िदान आप श [MASK]द नही [MASK]त [MASK]ग कि आपको एटरियल फ [MASK]बरिलशन [MASK] एएफ [MASK]ईबी ) [MASK] [SEP] [CLS] किसी अनय [MASK] स सवासथय जाच करान पर इस सथिति का पता चल सकता ह [SEP] [CLS] यह तवरित [MASK] दरद रहित परीकषण हदय की विदयत गतिविधि को मापता ह [SEP] [CLS] इलकटरोड नामक [MASK]िपचिप पच छाती और कभी - कभी [MASK]ाहो [MASK] परो पर लगाए [MASK] ह [SEP] [CLS] तार [MASK]礼टरালিত [MASK] कपयटर'\n",
      ">>> स जोडत ह [MASK] जो परीकषण पर [MASK]णामो को [MASK]िट या परदरशित करता ह [SEP] [CLS] जब आप अपनी नियमित गतिविधि [MASK] 蛆 ह तो इस एक या [MASK] दिन क [MASK] पहन [MASK] जाता ह [SEP] [CLS] अनियमित 絆दय [MASK]ाल [MASK] [MASK] चलन पर कछ उपकरण सवचालित रप स रिकॉरड करत ह [SEP] [CLS] यह उपकरण तीन साल तक लगातार दिल की धडक [MASK] को [MASK]िकॉरड करता ह [SEP] [CLS] डिवाइस दिखाता ह कि जब [MASK] [MASK] [MASK] दनिक गतिवि'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\">>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "from transformers import default_data_collator\n",
    "\n",
    "def whole_word_masking_data_collator(features, wwm_probability = 0.2):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        feature[\"labels\"] = new_labels\n",
    "\n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [CLS] நோய [MASK] [MASK] [MASK] [MASK] உஙகளுககு ஏடரியல ஃபைபரிலேஷன afib இருபபது [MASK] [MASK] [MASK] [MASK] தெரியாது [SEP] [CLS] மறறொரு காரணததிறகாக [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] செயயபபடும போது இநத நிலை கணடறியபபடலாம [SEP] [CLS] இநத விரைவான மறறும வலியறற சோதனை இதயததின மின செயலபாடடை அளவிடுகிறது [SEP] [CLS] மினமுனைகள எனபபடும ஒடடும இணைபபுகள மாரபிலும சில [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]\n",
      ">>> காலகளிலும வைககபபடுகினறன [SEP] [CLS] கமபிகள மினமுனைகளை கணினியுடன இணைககினறன, [MASK] சோதனை முடிவுகளை அசசிடுகிறது அலலது காணபிககும [SEP] [CLS] உஙகள [MASK] [MASK] [MASK] [MASK] செயலபாடுகளைச [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] ஒனறு [MASK] இரணடு நாடகளுககு அணியபபடும [SEP] [CLS] ஒழுஙகறற இதயத துடிபபு [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] சில சாதனஙகள தானாகவே பதிவு [MASK] [MASK] [MASK] [MASK] [SEP] [CLS] இநத [MASK] [MASK] [MASK] மூனறு ஆணடுகள வரை [MASK] [MASK] [MASK] து\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\">>> {tokenizer.decode(chunk)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 25\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 0.9\n",
    "test_size = 0.1\n",
    "\n",
    "downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=42\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned\",\n",
    "    # overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571cd25d2c4c4f77954dfd39527af3d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ba4d73b66c43b984d8cc4c8a0747cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0333046913146973, 'eval_runtime': 0.1539, 'eval_samples_per_second': 19.496, 'eval_steps_per_second': 6.499, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "442e6d7fd33f4372b82a2af4a82485d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.349650502204895, 'eval_runtime': 0.1331, 'eval_samples_per_second': 22.533, 'eval_steps_per_second': 7.511, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e2c14913a324e7dbb3ef2dcbd90f327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8491617441177368, 'eval_runtime': 0.148, 'eval_samples_per_second': 20.273, 'eval_steps_per_second': 6.758, 'epoch': 3.0}\n",
      "{'train_runtime': 32.11, 'train_samples_per_second': 2.336, 'train_steps_per_second': 0.093, 'train_loss': 1.6208845774332683, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=1.6208845774332683, metrics={'train_runtime': 32.11, 'train_samples_per_second': 2.336, 'train_steps_per_second': 0.093, 'train_loss': 1.6208845774332683, 'epoch': 3.0})"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask_filler = pipeline(\n",
    "    \"fill-mask\", model=model, tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "diagnosis you may not know you have atrial fibrillation afib\n",
      "-----\n",
      "diagnosis you do not know you have atrial fibrillation afib\n",
      "-----\n",
      "diagnosis you may not believe you have atrial fibrillation afib\n",
      "-----\n",
      "diagnosis you may not know you are atrial fibrillation afib\n",
      "-----\n",
      "diagnosis you may not know you have a fibrillation afib\n",
      "-----\n",
      "diagnosis you may not know you have atrial. afib\n"
     ]
    }
   ],
   "source": [
    "# Diagnosis You may not know you have atrial fibrillation AFib\n",
    "test_sentences =[\"Diagnosis [MASK] may not know you have atrial fibrillation AFib\",\n",
    "                 \"Diagnosis You [MASK] not know you have atrial fibrillation AFib\",\n",
    "                 \"Diagnosis You may not [MASK] you have atrial fibrillation AFib\",\n",
    "                 \"Diagnosis You may not know you [MASK] atrial fibrillation AFib\",\n",
    "                 \"Diagnosis You may not know you have [MASK] fibrillation AFib\",\n",
    "                 \"Diagnosis You may not know you have atrial [MASK] AFib\"]\n",
    "\n",
    "for text in test_sentences:\n",
    "    preds = mask_filler(text)\n",
    "    print(\"-----\")\n",
    "    # for pred in preds:\n",
    "    #     print(f\"> {pred['sequence']}\")\n",
    "    print(preds[0]['sequence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \t निदान आप शायद नहीं जानते होंगे कि आपको एट्रियल फ़िब्रिलेशन (एएफआईबी) है\n",
      "\n",
      "[{'score': 0.22944827377796173, 'token': 29876, 'token_str': '##ा', 'sequence': '[CLS] निदान आप शायदा जानत होग कि [MASK] एटरियल [UNK] ( [UNK] ) ह. [SEP]'}, {'score': 0.17568902671337128, 'token': 29877, 'token_str': '##ि', 'sequence': '[CLS] निदान आप शायदि जानत होग कि [MASK] एटरियल [UNK] ( [UNK] ) ह. [SEP]'}, {'score': 0.17328883707523346, 'token': 29878, 'token_str': '##ी', 'sequence': '[CLS] निदान आप शायदी जानत होग कि [MASK] एटरियल [UNK] ( [UNK] ) ह. [SEP]'}, {'score': 0.11199919134378433, 'token': 29869, 'token_str': '##र', 'sequence': '[CLS] निदान आप शायदर जानत होग कि [MASK] एटरियल [UNK] ( [UNK] ) ह. [SEP]'}, {'score': 0.08817306905984879, 'token': 29863, 'token_str': '##न', 'sequence': '[CLS] निदान आप शायदन जानत होग कि [MASK] एटरियल [UNK] ( [UNK] ) ह. [SEP]'}]\n",
      "[{'score': 0.21659012138843536, 'token': 29859, 'token_str': '##त', 'sequence': '[CLS] निदान आप शायद [MASK] जानत होग कित एटरियल [UNK] ( [UNK] ) ह. [SEP]'}, {'score': 0.12410874664783478, 'token': 29869, 'token_str': '##र', 'sequence': '[CLS] निदान आप शायद [MASK] जानत होग किर एटरियल [UNK] ( [UNK] ) ह. [SEP]'}, {'score': 0.11347272247076035, 'token': 29870, 'token_str': '##ल', 'sequence': '[CLS] निदान आप शायद [MASK] जानत होग किल एटरियल [UNK] ( [UNK] ) ह. [SEP]'}, {'score': 0.10133595764636993, 'token': 29863, 'token_str': '##न', 'sequence': '[CLS] निदान आप शायद [MASK] जानत होग किन एटरियल [UNK] ( [UNK] ) ह. [SEP]'}, {'score': 0.0844426080584526, 'token': 29851, 'token_str': '##क', 'sequence': '[CLS] निदान आप शायद [MASK] जानत होग किक एटरियल [UNK] ( [UNK] ) ह. [SEP]'}]\n"
     ]
    }
   ],
   "source": [
    "text = \"निदान आप शायद नहीं जानते होंगे कि आपको एट्रियल फ़िब्रिलेशन (एएफआईबी) है\"\n",
    "masked = \"निदान आप शायद [MASK] जानते होंगे कि [MASK] एट्रियल फ़िब्रिलेशन (एएफआईबी) है.\"\n",
    "\n",
    "preds = mask_filler(masked)\n",
    "\n",
    "\n",
    "print(f\"Original text: \\t {text}\\n\")\n",
    "\n",
    "for pred in preds:\n",
    "    # print(f\"\\t >{pred['sequence']}\")\n",
    "    print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \t Der Zustand kann festgestellt werden, wenn aus einem anderen Grund eine Gesundheitsuntersuchung durchgeführt wird\n",
      "\n",
      "{'score': 0.2608787417411804, 'token': 2063, 'token_str': '##e', 'sequence': 'der zustande festgestellt werden, wenn aus einem anderen grund eine gesundheitsuntersuchung durchgefuhrt wird.'}\n",
      "{'score': 0.14552222192287445, 'token': 27412, 'token_str': '##liche', 'sequence': 'der zustandliche festgestellt werden, wenn aus einem anderen grund eine gesundheitsuntersuchung durchgefuhrt wird.'}\n",
      "{'score': 0.058971669524908066, 'token': 2368, 'token_str': '##en', 'sequence': 'der zustanden festgestellt werden, wenn aus einem anderen grund eine gesundheitsuntersuchung durchgefuhrt wird.'}\n",
      "{'score': 0.05674171447753906, 'token': 27665, 'token_str': 'eine', 'sequence': 'der zustand eine festgestellt werden, wenn aus einem anderen grund eine gesundheitsuntersuchung durchgefuhrt wird.'}\n",
      "{'score': 0.044515468180179596, 'token': 4315, 'token_str': 'der', 'sequence': 'der zustand der festgestellt werden, wenn aus einem anderen grund eine gesundheitsuntersuchung durchgefuhrt wird.'}\n"
     ]
    }
   ],
   "source": [
    "text = \"Der Zustand kann festgestellt werden, wenn aus einem anderen Grund eine Gesundheitsuntersuchung durchgeführt wird\"\n",
    "masked = \"Der Zustand [MASK] festgestellt werden, wenn aus einem anderen Grund eine Gesundheitsuntersuchung durchgeführt wird.\"\n",
    "\n",
    "preds = mask_filler(masked)\n",
    "\n",
    "\n",
    "print(f\"Original text: \\t {text}\\n\")\n",
    "\n",
    "for pred in preds:\n",
    "    # print(f\"\\t >{pred['sequence']}\")\n",
    "    print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \t يمكن اكتشاف الحالة عند إجراء فحص صحي لسبب آخر\n",
      "\n",
      "{'score': 0.21959923207759857, 'token': 1268, 'token_str': '،', 'sequence': 'يمكن اكتشاف الحالة عند اجراء ، صحي لسبب اخر'}\n",
      "{'score': 0.1942230463027954, 'token': 1270, 'token_str': 'ا', 'sequence': 'يمكن اكتشاف الحالة عند اجراء ا صحي لسبب اخر'}\n",
      "{'score': 0.1261647790670395, 'token': 1298, 'token_str': 'و', 'sequence': 'يمكن اكتشاف الحالة عند اجراء و صحي لسبب اخر'}\n",
      "{'score': 0.0734768807888031, 'token': 19433, 'token_str': '##ة', 'sequence': 'يمكن اكتشاف الحالة عند اجراءة صحي لسبب اخر'}\n",
      "{'score': 0.03801721706986427, 'token': 1294, 'token_str': 'ل', 'sequence': 'يمكن اكتشاف الحالة عند اجراء ل صحي لسبب اخر'}\n"
     ]
    }
   ],
   "source": [
    "# ara\n",
    "text = \"يمكن اكتشاف الحالة عند إجراء فحص صحي لسبب آخر\"\n",
    "masked = \"يمكن اكتشاف الحالة عند إجراء [MASK] صحي لسبب آخر\"\n",
    "\n",
    "preds = mask_filler(masked)\n",
    "\n",
    "\n",
    "print(f\"Original text: \\t {text}\\n\")\n",
    "\n",
    "for pred in preds:\n",
    "    # print(f\"\\t >{pred['sequence']}\")\n",
    "    print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \t À quelle fréquence dois-je subir un test de dépistage d'une maladie cardiaque ou de complications liées à la fibrillation auriculaire\n",
      "\n",
      "{'score': 0.05648840218782425, 'token': 25207, 'token_str': 'nouvelle', 'sequence': \"a quelle frequence dois - je subir un test de depistage d'une nouvelle cardiaque ou de complications liees a la fibrillation auriculaire\"}\n",
      "{'score': 0.036004677414894104, 'token': 2139, 'token_str': 'de', 'sequence': \"a quelle frequence dois - je subir un test de depistage d'une de cardiaque ou de complications liees a la fibrillation auriculaire\"}\n",
      "{'score': 0.02528570033609867, 'token': 2474, 'token_str': 'la', 'sequence': \"a quelle frequence dois - je subir un test de depistage d'une la cardiaque ou de complications liees a la fibrillation auriculaire\"}\n",
      "{'score': 0.01775709167122841, 'token': 3393, 'token_str': 'le', 'sequence': \"a quelle frequence dois - je subir un test de depistage d'une le cardiaque ou de complications liees a la fibrillation auriculaire\"}\n",
      "{'score': 0.016374090686440468, 'token': 9526, 'token_str': 'cure', 'sequence': \"a quelle frequence dois - je subir un test de depistage d'une cure cardiaque ou de complications liees a la fibrillation auriculaire\"}\n"
     ]
    }
   ],
   "source": [
    "# fre\n",
    "text = \"À quelle fréquence dois-je subir un test de dépistage d'une maladie cardiaque ou de complications liées à la fibrillation auriculaire\"\n",
    "masked = \"À quelle fréquence dois-je subir un test de dépistage d'une [MASK] cardiaque ou de complications liées à la fibrillation auriculaire\"\n",
    "\n",
    "preds = mask_filler(masked)\n",
    "\n",
    "\n",
    "print(f\"Original text: \\t {text}\\n\")\n",
    "\n",
    "for pred in preds:\n",
    "    # print(f\"\\t >{pred['sequence']}\")\n",
    "    print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \t 心臓病または心房細動の合併症の検査はどのくらいの頻度で受けるべきですか\n",
      "\n",
      "{'score': 0.2776617109775543, 'token': 30211, 'token_str': '##ら', 'sequence': '心 または 心 の 合 の はとのくら いの て けるへきてすか'}\n",
      "{'score': 0.13419713079929352, 'token': 30213, 'token_str': '##る', 'sequence': '心 または 心 の 合 の はとのくる いの て けるへきてすか'}\n",
      "{'score': 0.08000142872333527, 'token': 30191, 'token_str': '##て', 'sequence': '心 または 心 の 合 の はとのくて いの て けるへきてすか'}\n",
      "{'score': 0.0701446533203125, 'token': 30217, 'token_str': '##ん', 'sequence': '心 または 心 の 合 の はとのくん いの て けるへきてすか'}\n",
      "{'score': 0.041595570743083954, 'token': 30212, 'token_str': '##り', 'sequence': '心 または 心 の 合 の はとのくり いの て けるへきてすか'}\n"
     ]
    }
   ],
   "source": [
    "# jap\n",
    "text = \"心臓病または心房細動の合併症の検査はどのくらいの頻度で受けるべきですか\"\n",
    "masked = \"心臓病または心房細動の合併症の検査はどのく[MASK]いの頻度で受けるべきですか\"\n",
    "\n",
    "preds = mask_filler(masked)\n",
    "\n",
    "\n",
    "print(f\"Original text: \\t {text}\\n\")\n",
    "\n",
    "for pred in preds:\n",
    "    # print(f\"\\t >{pred['sequence']}\")\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \t 健康的生活方式是预防心脏问题和心脏相关并发症的主要保护措施\n",
      "\n",
      "{'score': 0.11782781779766083, 'token': 1849, 'token_str': '心', 'sequence': '健 的 生 方 心 和 心 相 的 主 保'}\n",
      "{'score': 0.11191239953041077, 'token': 1916, 'token_str': '的', 'sequence': '健 的 生 方 的 和 心 相 的 主 保'}\n",
      "{'score': 0.06738013029098511, 'token': 1740, 'token_str': '一', 'sequence': '健 的 生 方 一 和 心 相 的 主 保'}\n",
      "{'score': 0.05129873752593994, 'token': 1796, 'token_str': '和', 'sequence': '健 的 生 方 和 和 心 相 的 主 保'}\n",
      "{'score': 0.02993958815932274, 'token': 1855, 'token_str': '我', 'sequence': '健 的 生 方 我 和 心 相 的 主 保'}\n"
     ]
    }
   ],
   "source": [
    "# man\n",
    "text =   \"健康的生活方式是预防心脏问题和心脏相关并发症的主要保护措施\"\n",
    "masked = \"健康的生活方式是预防[MASK]脏问题和心脏相关并发症的主要保护措施\"\n",
    "\n",
    "preds = mask_filler(masked)\n",
    "\n",
    "\n",
    "print(f\"Original text: \\t {text}\\n\")\n",
    "\n",
    "for pred in preds:\n",
    "    # print(f\"\\t >{pred['sequence']}\")\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \t Czy zawsze masz objawy, czy pojawiają się i znikają?\n",
      "\n",
      "{'score': 0.04858575388789177, 'token': 23564, 'token_str': 'za', 'sequence': 'czy zawsze masz objawy, czy za sie i znikaja?'}\n",
      "{'score': 0.046745702624320984, 'token': 2213, 'token_str': '##m', 'sequence': 'czy zawsze masz objawy, czym sie i znikaja?'}\n",
      "{'score': 0.038410332053899765, 'token': 1062, 'token_str': 'z', 'sequence': 'czy zawsze masz objawy, czy z sie i znikaja?'}\n",
      "{'score': 0.029569320380687714, 'token': 1010, 'token_str': ',', 'sequence': 'czy zawsze masz objawy, czy, sie i znikaja?'}\n",
      "{'score': 0.027179347351193428, 'token': 2480, 'token_str': '##z', 'sequence': 'czy zawsze masz objawy, czyz sie i znikaja?'}\n"
     ]
    }
   ],
   "source": [
    "# pol\n",
    "text =   \"Czy zawsze masz objawy, czy pojawiają się i znikają?\"\n",
    "masked = \"Czy zawsze masz objawy, czy [MASK] się i znikają?\"\n",
    "\n",
    "preds = mask_filler(masked)\n",
    "\n",
    "\n",
    "print(f\"Original text: \\t {text}\\n\")\n",
    "\n",
    "for pred in preds:\n",
    "    # print(f\"\\t >{pred['sequence']}\")\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \t நான் பின்பற்ற வேண்டிய வேறு ஏதேனும் கட்டுப்பாடுகள் உள்ளனவா\n",
      "\n",
      "{'score': 0.15485689043998718, 'token': 39216, 'token_str': '##தாக', 'sequence': 'நான பினபறற வேணடியதாக ஏதேனும கடடுபபாடுகள உளளனவா'}\n",
      "{'score': 0.06965184956789017, 'token': 16112, 'token_str': '##ால', 'sequence': 'நான பினபறற வேணடியால ஏதேனும கடடுபபாடுகள உளளனவா'}\n",
      "{'score': 0.05663551762700081, 'token': 79264, 'token_str': 'அனைதது', 'sequence': 'நான பினபறற வேணடிய அனைதது ஏதேனும கடடுபபாடுகள உளளனவா'}\n",
      "{'score': 0.042365241795778275, 'token': 14814, 'token_str': '##து', 'sequence': 'நான பினபறற வேணடியது ஏதேனும கடடுபபாடுகள உளளனவா'}\n",
      "{'score': 0.03979815915226936, 'token': 49683, 'token_str': '##வரகள', 'sequence': 'நான பினபறற வேணடியவரகள ஏதேனும கடடுபபாடுகள உளளனவா'}\n"
     ]
    }
   ],
   "source": [
    "# tam\n",
    "text =   \"நான் பின்பற்ற வேண்டிய வேறு ஏதேனும் கட்டுப்பாடுகள் உள்ளனவா\"\n",
    "masked = \"நான் பின்பற்ற வேண்டிய [MASK] ஏதேனும் கட்டுப்பாடுகள் உள்ளனவா\"\n",
    "\n",
    "preds = mask_filler(masked)\n",
    "\n",
    "\n",
    "print(f\"Original text: \\t {text}\\n\")\n",
    "\n",
    "for pred in preds:\n",
    "    # print(f\"\\t >{pred['sequence']}\")\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
